{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RAG without State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(all_splits,embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_type = \"similarity\",search_kwargs = {\"k\":3})\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\",\"You are a helpful assistant who provide response based on the given {context}\"),\n",
    "    (\"user\",\"{query}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain  = (\n",
    "    {\"context\":retriever | format_docs,\"query\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a process used in planning where a complicated task is broken down into smaller, more manageable steps. This approach helps an agent understand the various components of a complex task and plan effectively to accomplish it. In the context of LLM-powered autonomous agents, Task Decomposition often involves the use of the \"Chain of Thought\" (CoT) technique, which prompts the model to \"think step by step.\" This technique enhances the model's performance by allowing it to utilize more computation during test time to transform big tasks into multiple manageable tasks, providing insight into the modelâ€™s thinking process."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a process used in planning where a complicated task, which usually involves many steps, is broken down into smaller and simpler steps. This approach helps an agent understand the steps involved in a task and plan ahead effectively. A common technique for task decomposition in the context of using large language models (LLMs) is the Chain of Thought (CoT) prompting. This technique instructs the model to \"think step by step,\" allowing it to utilize more computation during test time to transform big tasks into multiple manageable tasks. This not only enhances the model\\'s performance on complex tasks but also provides insight into the model\\'s thinking process.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RAG with State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the sub-chain that reformulates the query based on the chat history.\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contexualized_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "sub_chain  = contexualized_prompt | llm | StrOutputParser()\n",
    "rag_chain_with_history = sub_chain | rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The different methods for performing task decomposition are:\\n\\n1. **By LLM with Simple Prompting**: This involves using language models with prompts such as \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" to break down the task into smaller, manageable steps.\\n\\n2. **Using Task-Specific Instructions**: This method uses specific instructions tailored to the task at hand. For example, for writing a novel, the instruction might be \"Write a story outline.\"\\n\\n3. **With Human Inputs**: This involves human intervention to decompose the task, where a person manually breaks down the task into sub-tasks or steps.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "question =  \"What is Task decompositiion\"\n",
    "\n",
    "response = rag_chain_with_history.invoke(\n",
    "    { \"chat_history\":[],\n",
    "     \"query\":question}\n",
    "     )\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain_with_history.invoke({\"chat_history\":chat_history,\n",
    "                               \"query\": \"What are the ways of doing it?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# We define a dict representing the state of the application.\n",
    "# This state has the same input and output keys as `rag_chain`.\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    #context: str\n",
    "\n",
    "\n",
    "# We then define a simple node that runs the `rag_chain`.\n",
    "# The `return` values of the node update the graph state, so here we just\n",
    "# update the chat history with the input message and response.\n",
    "def call_model(state: State):\n",
    "    response = rag_chain_with_history.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"query\"]),\n",
    "            AIMessage(response),\n",
    "        ],\n",
    "      # \"context\": response[\"context\"],\n",
    "    # \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Our graph consists only of one node:\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Finally, we compile the graph with a checkpointer object.\n",
    "# This persists the state, in this case in memory.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "result = app.invoke(\n",
    "    {\"query\": \"What is Task Decomposition?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Task Decomposition?',\n",
       " 'chat_history': [HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}, id='67641b43-d558-4d16-8b32-f609a1ed44d5'),\n",
       "  AIMessage(content='Task decomposition refers to the process of breaking down a complex task into smaller, more manageable steps. This approach helps to simplify the task, making it easier to tackle and solve. In the context of large language models (LLMs) and autonomous agents, task decomposition is often achieved using techniques like Chain of Thought (CoT) prompting. CoT instructs the model to \"think step by step,\" allowing it to utilize more computation time during testing to decompose difficult tasks and provide a clearer interpretation of its reasoning process.', additional_kwargs={}, response_metadata={}, id='638d2620-4394-4d95-a2c5-be8c7a8e74e0')]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"query\": \"What is Task Decomposition?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"query\": \"What is one way of doing it?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is one way of doing it?',\n",
       " 'chat_history': [HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}, id='67641b43-d558-4d16-8b32-f609a1ed44d5'),\n",
       "  AIMessage(content='Task decomposition refers to the process of breaking down a complex task into smaller, more manageable steps. This approach helps to simplify the task, making it easier to tackle and solve. In the context of large language models (LLMs) and autonomous agents, task decomposition is often achieved using techniques like Chain of Thought (CoT) prompting. CoT instructs the model to \"think step by step,\" allowing it to utilize more computation time during testing to decompose difficult tasks and provide a clearer interpretation of its reasoning process.', additional_kwargs={}, response_metadata={}, id='638d2620-4394-4d95-a2c5-be8c7a8e74e0'),\n",
       "  HumanMessage(content='What is one way of doing it?', additional_kwargs={}, response_metadata={}, id='766acfb2-3365-4612-9a06-68dbed239d0f'),\n",
       "  AIMessage(content='One method for performing task decomposition is the \"Chain of Thought\" (CoT) technique. This technique involves prompting the model to \"think step by step,\" which helps decompose complex tasks into smaller and simpler steps.', additional_kwargs={}, response_metadata={}, id='4fda5dd5-2d7c-45f7-8b97-0e4e042651c1')]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verloopAssignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
